{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 2\n",
    "--\n",
    "\n",
    "Names: Kaan Tural, Arinjay Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Training your own word embeddings (14 points)\n",
    "--------------------------------\n",
    "\n",
    "For this task, you'll use the `gensim` package to train your own embeddings for both words and characters. These will eventually act as inputs to your neural language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/turalk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/var/folders/sh/9y2x7hyx76bghklsygtm6m000000gn/T/ipykernel_36377/260011025.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "import pandas as pd\n",
    "\n",
    "# for word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# your other imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on colab, you'll need to mount your drive to access data files\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "# The dimensions of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# DO NOT WRITE \"50\" WHEN YOU ARE REFERRING TO THE EMBEDDING SIZE\n",
    "EMBEDDINGS_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embeddings on provided dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 Sentences: ['This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'\n",
      " 'It never once occurred to me that the fumbling might be a mere mistake.']\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "By Char:  [['<s>', '<s>', 't', 'h', 'i', 's', '_', 'p', 'r', 'o', 'c', 'e', 's', 's', ',', '_', 'h', 'o', 'w', 'e', 'v', 'e', 'r', ',', '_', 'a', 'f', 'f', 'o', 'r', 'd', 'e', 'd', '_', 'm', 'e', '_', 'n', 'o', '_', 'm', 'e', 'a', 'n', 's', '_', 'o', 'f', '_', 'a', 's', 'c', 'e', 'r', 't', 'a', 'i', 'n', 'i', 'n', 'g', '_', 't', 'h', 'e', '_', 'd', 'i', 'm', 'e', 'n', 's', 'i', 'o', 'n', 's', '_', 'o', 'f', '_', 'm', 'y', '_', 'd', 'u', 'n', 'g', 'e', 'o', 'n', ';', '_', 'a', 's', '_', 'i', '_', 'm', 'i', 'g', 'h', 't', '_', 'm', 'a', 'k', 'e', '_', 'i', 't', 's', '_', 'c', 'i', 'r', 'c', 'u', 'i', 't', ',', '_', 'a', 'n', 'd', '_', 'r', 'e', 't', 'u', 'r', 'n', '_', 't', 'o', '_', 't', 'h', 'e', '_', 'p', 'o', 'i', 'n', 't', '_', 'w', 'h', 'e', 'n', 'c', 'e', '_', 'i', '_', 's', 'e', 't', '_', 'o', 'u', 't', ',', '_', 'w', 'i', 't', 'h', 'o', 'u', 't', '_', 'b', 'e', 'i', 'n', 'g', '_', 'a', 'w', 'a', 'r', 'e', '_', 'o', 'f', '_', 't', 'h', 'e', '_', 'f', 'a', 'c', 't', ';', '_', 's', 'o', '_', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '_', 'u', 'n', 'i', 'f', 'o', 'r', 'm', '_', 's', 'e', 'e', 'm', 'e', 'd', '_', 't', 'h', 'e', '_', 'w', 'a', 'l', 'l', '.', '</s>', '</s>'], ['<s>', '<s>', 'i', 't', '_', 'n', 'e', 'v', 'e', 'r', '_', 'o', 'n', 'c', 'e', '_', 'o', 'c', 'c', 'u', 'r', 'r', 'e', 'd', '_', 't', 'o', '_', 'm', 'e', '_', 't', 'h', 'a', 't', '_', 't', 'h', 'e', '_', 'f', 'u', 'm', 'b', 'l', 'i', 'n', 'g', '_', 'm', 'i', 'g', 'h', 't', '_', 'b', 'e', '_', 'a', '_', 'm', 'e', 'r', 'e', '_', 'm', 'i', 's', 't', 'a', 'k', 'e', '.', '</s>', '</s>']]\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "By Word:  [['<s>', '<s>', 'this', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'i', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'i', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.', '</s>', '</s>'], ['<s>', '<s>', 'it', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.', '</s>', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# use the provided utility functions to read in the data\n",
    "\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "train = pd.read_csv(TRAIN_FILE)\n",
    "# read the data in both by character and by word\n",
    "by_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "by_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\n",
    "# print out the first two sentences in each format\n",
    "print(\"First 2 Sentences:\", train.text.values[:2])\n",
    "print(\"=\"*400)\n",
    "print(\"By Char: \", by_char[:2])\n",
    "print(\"=\"*400)\n",
    "print(\"By Word: \", by_word[:2])\n",
    "# make sure we can read the output easily, but you are welcome to leave tokens in their lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "# 10 points\n",
    "# create your word embeddings\n",
    "# use the skip gram algorithm and a window size of 5\n",
    "# min_count should be 1\n",
    "# typically takes ~3 sec on our computer for character embeddings using skip-gram with window size 5\n",
    "# typically takes ~3 sec on our computer for word embeddings using skip-gram with window size 5 \n",
    "\n",
    "print(len(by_word))\n",
    "\n",
    "#By Word\n",
    "modelWord = Word2Vec(sentences=by_word, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "modelWord.save(\"word2vec.model\")\n",
    "\n",
    "#By Char\n",
    "modelChar = Word2Vec(sentences=by_char, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "modelChar.save(\"char2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('instance', 0.9369615316390991), ('description', 0.9247301816940308), ('reasons', 0.9237263202667236), ('entertained', 0.918633222579956), ('travelling', 0.9183027148246765)]\n",
      "====================================================================================================================================================================================\n",
      "0.776537\n"
     ]
    }
   ],
   "source": [
    "print(modelWord.wv.most_similar('example', topn=5))\n",
    "\n",
    "print('='*180)\n",
    "\n",
    "print(modelChar.wv.similarity('.', '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for character embeddings is 60\n",
      "Vocabulary size for word embeddings is 25374\n"
     ]
    }
   ],
   "source": [
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n",
    "\n",
    "# Vocabulary size for character embeddings is 60\n",
    "# Vocabulary size for word embeddings is 25374\n",
    "\n",
    "print(\"Vocabulary size for character embeddings is\", len(modelChar.wv.index_to_key))\n",
    "print(\"Vocabulary size for word embeddings is\", len(modelWord.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both sets of embeddings in txt format\n",
    "# use the save_word2vec_format method to accomplish this, with the flag binary=False\n",
    "# you will load your embeddings from files later in the assignment\n",
    "# and can do so whenever you want to not re-train your embeddings\n",
    "# look at these files to make sure that you understand their format\n",
    "\n",
    "# Saving file in txt format. This will be used later in Sections 4 and 5\n",
    "# make it so that you don't have to re-train the embeddings each time\n",
    "\n",
    "modelChar.wv.save_word2vec_format(EMBEDDING_SAVE_FILE_CHAR, binary=False)\n",
    "modelWord.wv.save_word2vec_format(EMBEDDING_SAVE_FILE_WORD, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of index of embeddings for character is 60\n",
      "Size of index of embeddings for word is 25374\n"
     ]
    }
   ],
   "source": [
    "#size of index of embeddings\n",
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "print(\"Size of index of embeddings for character is\", len(modelChar.wv.index_to_key))\n",
    "print(\"Size of index of embeddings for word is\", len(modelWord.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
